name: Restore E2E S3 Verify

on:
  workflow_dispatch:
    inputs:
      use_inmemory:
        description: 'Run test using in-memory MongoDB (skips Docker start)'
        required: false
        default: 'false'

jobs:
  restore-e2e:
    runs-on: ubuntu-latest
    steps:
      - name: Check required secrets (or skip when using in-memory)
        run: |
          if [ "${{ github.event.inputs.use_inmemory }}" = "true" ]; then
            echo "use_inmemory requested; skipping initial AWS/S3 secret presence check. Note: upload/download steps still require secrets."
          else
            if [ -z "${{ secrets.AWS_ACCESS_KEY_ID }}" ] || [ -z "${{ secrets.AWS_SECRET_ACCESS_KEY }}" ] || [ -z "${{ secrets.S3_BUCKET }}" ]; then
              echo "Required AWS secrets or S3_BUCKET not provided. Set AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and S3_BUCKET in repo secrets to run this workflow."
              exit 1
            fi
          fi

      - name: Checkout
        uses: actions/checkout@v4

      - name: Install AWS CLI
        run: |
          sudo apt-get update && sudo apt-get install -y unzip
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install

      - name: Configure AWS credentials
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_REGION }}
        run: |
          aws sts get-caller-identity

      - name: Run backup locally (Docker or in-memory)
        run: |
          mkdir -p e2e-backups
          export MONGO_URI=mongodb://localhost:27017/hafjet-bukku-e2e
          if [ "${{ github.event.inputs.use_inmemory }}" = "true" ]; then
            echo "Using in-memory MongoDB for test (use_inmemory=true)"
            export USE_INMEMORY=true
            node backend/scripts/test-backup-restore.js
          else
            echo "Starting Docker MongoDB for test"
            docker run -d --name e2e-mongo -p 27017:27017 mongo:7
            sleep 5
            node backend/scripts/test-backup-restore.js || true
            docker stop e2e-mongo || true
            docker rm e2e-mongo || true
          fi

      - name: Find backup and upload to S3
        id: upload-s3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          S3_BUCKET: ${{ secrets.S3_BUCKET }}
        run: |
          # Check if secrets are available
          if [ -z "$AWS_ACCESS_KEY_ID" ] || [ -z "$AWS_SECRET_ACCESS_KEY" ] || [ -z "$S3_BUCKET" ]; then
            echo "âš ï¸ AWS secrets not configured - skipping S3 upload"
            echo "ðŸ’¡ Add AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and S3_BUCKET secrets to enable S3 backup"
            exit 0
          fi
          
          BACKUP_DIR=$(ls -td backend/backups/* | head -n1)
          if [ -z "$BACKUP_DIR" ]; then echo "No backup found"; exit 1; fi
          echo "Uploading $BACKUP_DIR to s3://$S3_BUCKET/backups/"
          tar -czf backup.tgz -C "$BACKUP_DIR" .
          aws s3 cp backup.tgz s3://$S3_BUCKET/backups/backup-$(date -u +%Y%m%dT%H%M%SZ).tgz

      - name: Download latest backup and restore
        id: restore-s3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          S3_BUCKET: ${{ secrets.S3_BUCKET }}
        run: |
          # Check if secrets are available
          if [ -z "$AWS_ACCESS_KEY_ID" ] || [ -z "$AWS_SECRET_ACCESS_KEY" ] || [ -z "$S3_BUCKET" ]; then
            echo "âš ï¸ AWS secrets not configured - skipping S3 restore"
            exit 0
          fi
          
          aws s3 ls s3://$S3_BUCKET/backups/ --recursive | sort | tail -n1 | awk '{print $4}' > latest.txt
          LATEST=$(cat latest.txt)
          if [ -z "$LATEST" ]; then echo "No backup in bucket"; exit 1; fi
          echo "Downloading $LATEST"
          aws s3 cp s3://$S3_BUCKET/$LATEST latest.tgz
          mkdir -p restore_dir
          tar -xzf latest.tgz -C restore_dir
          # Start a fresh mongo and restore
          docker run -d --name e2e-restore-mongo -p 27017:27017 mongo:7
          sleep 5
          mongorestore --drop --gzip --archive=restore_dir || true
          # Basic verification: list databases
          mongosh --eval "db.adminCommand('listDatabases')"
          docker stop e2e-restore-mongo || true
          docker rm e2e-restore-mongo || true

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: e2e-backup-artifacts
          path: |
            backup.tgz
            latest.tgz
            backend/backups/**
          if-no-files-found: warn
